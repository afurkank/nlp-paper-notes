# Abstract
* The paper identifies that small language models struggle to produce coherent and fluent text.
* Introduces "TinyStories," a dataset of simple short stories generated by advanced models like GPT-3.5 and GPT-4.
* Demonstrates that TinyStories can train smaller models (below 10 million parameters) to produce high-quality, grammatically correct text.
* Introduces a new evaluation paradigm that uses GPT-4 to grade the text generated by other models, mimicking a human teacher's grading process.
* The new grading framework provides a multidimensional score, evaluating grammar, creativity, and instruction-following.
* Aims to facilitate the development and research of language models, especially for low-resource or specialized applications.
* Hopes to provide insights into how language capabilities emerge in language models.

***
## Section 3.1 - Insights
* The new evaluation method offers a fine-grained assessment of a model's capabilities, showing how different abilities depend on the model's size and architecture.
* Shallower models perform better in terms of grammar but are less consistent in content, indicating that model depth is crucial for content consistency.
* Grammar scores plateau earlier than other evaluation metrics, suggesting that smaller models can master grammar while larger sizes are needed for consistency and creativity.
* The ability to produce consistent text begins to emerge when the model's hidden size increases from 64 to 128.
* The largest model trained on TinyStories (around 80M parameters) scores almost perfectly in grammar and consistency but lags behind GPT-4 in creativity.
* Models with only one layer struggle with instruction-following, which likely relies on global attention. Two layers appear sufficient for some degree of instruction-following.
* The quality of instruction-following depends more on the number of layers, while the coherence of the plot is more influenced by the hidden dimension size.

***
## Dataset
https://huggingface.co/datasets/roneneldan/TinyStories
