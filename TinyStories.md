# TinyStories

***

## Abstract
* The paper identifies that small language models struggle to produce coherent and fluent text.
* Introduces "TinyStories," a dataset of simple short stories generated by advanced models like GPT-3.5 and GPT-4.
* Demonstrates that TinyStories can train smaller models (below 10 million parameters) to produce high-quality, grammatically correct text.
* Introduces a new evaluation paradigm that uses GPT-4 to grade the text generated by other models, mimicking a human teacher's grading process.
* The new grading framework provides a multidimensional score, evaluating grammar, creativity, and instruction-following.
* Aims to facilitate the development and research of language models, especially for low-resource or specialized applications.
* Hopes to provide insights into how language capabilities emerge in language models.

***

## Section 3.1 - Insights
* The new evaluation method offers a fine-grained assessment of a model's capabilities, showing how different abilities depend on the model's size and architecture.
* Shallower models perform better in terms of grammar but are less consistent in content, indicating that model depth is crucial for content consistency.
* Grammar scores plateau earlier than other evaluation metrics, suggesting that smaller models can master grammar while larger sizes are needed for consistency and creativity.
* The ability to produce consistent text begins to emerge when the model's hidden size increases from 64 to 128.
* The largest model trained on TinyStories (around 80M parameters) scores almost perfectly in grammar and consistency but lags behind GPT-4 in creativity.
* Models with only one layer struggle with instruction-following, which likely relies on global attention. Two layers appear sufficient for some degree of instruction-following.
* The quality of instruction-following depends more on the number of layers, while the coherence of the plot is more influenced by the hidden dimension size.

***

## Evaluation Results

![tiny1](https://github.com/afurkank/nlp-paper-notes/assets/62884181/4db8c99b-859a-4b05-a40c-53edcc9f60a4)


***

## Completion of Different Models

![tiny2](https://github.com/afurkank/nlp-paper-notes/assets/62884181/d89f6f24-bf32-48e4-8df6-b587c2990c62)


***

## Completion of Different Models - 2

![tiny3](https://github.com/afurkank/nlp-paper-notes/assets/62884181/b14c72c7-a823-4f17-a192-36cb912b39d1)


***

## Model Performance w/ Different Layers and Heads

![tiny4](https://github.com/afurkank/nlp-paper-notes/assets/62884181/0bde76aa-39dd-414e-8b61-ea6deb3ac043)


***

## Data
The dataset has 2.4M stories. When tokenized with "bert-base-uncased", it produces ~450M tokens.
A model with ~20M parameters could benefit fully from this dataset. Bigger than this would need more than TinyStories dataset and smaller than this would not need that big of a dataset(According to Chinchilla scaling law).

***

## Dataset
https://huggingface.co/datasets/roneneldan/TinyStories

***

Paper link: https://arxiv.org/pdf/2305.07759.pdf
