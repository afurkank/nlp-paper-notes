# Abstract
* The paper identifies that small language models struggle to produce coherent and fluent text.
* Introduces "TinyStories," a dataset of simple short stories generated by advanced models like GPT-3.5 and GPT-4.
* Demonstrates that TinyStories can train smaller models (below 10 million parameters) to produce high-quality, grammatically correct text.
* Introduces a new evaluation paradigm that uses GPT-4 to grade the text generated by other models, mimicking a human teacher's grading process.
* The new grading framework provides a multidimensional score, evaluating grammar, creativity, and instruction-following.
* Aims to facilitate the development and research of language models, especially for low-resource or specialized applications.
* Hopes to provide insights into how language capabilities emerge in language models.

***

## Section 3.1 - Insights
* The new evaluation method offers a fine-grained assessment of a model's capabilities, showing how different abilities depend on the model's size and architecture.
* Shallower models perform better in terms of grammar but are less consistent in content, indicating that model depth is crucial for content consistency.
* Grammar scores plateau earlier than other evaluation metrics, suggesting that smaller models can master grammar while larger sizes are needed for consistency and creativity.
* The ability to produce consistent text begins to emerge when the model's hidden size increases from 64 to 128.
* The largest model trained on TinyStories (around 80M parameters) scores almost perfectly in grammar and consistency but lags behind GPT-4 in creativity.
* Models with only one layer struggle with instruction-following, which likely relies on global attention. Two layers appear sufficient for some degree of instruction-following.
* The quality of instruction-following depends more on the number of layers, while the coherence of the plot is more influenced by the hidden dimension size.

***

## Evaluation Results

![tinystories](https://github.com/afurkank/paper-notes/assets/62884181/15c6a689-08a9-496c-bb07-c2577fa9ef05)

***

## Completion of Different Models

![tinystories2](https://github.com/afurkank/paper-notes/assets/62884181/bd6cd8e1-db8b-4e93-b21e-c99a55ff8257)

***

## Completion of Different Models - 2

![tinystories3](https://github.com/afurkank/paper-notes/assets/62884181/847febef-019b-4e06-bac0-e5ee8e7bcfa9)

***

## Model Performance w/ Different Layers and Heads

![tinystories4](https://github.com/afurkank/paper-notes/assets/62884181/59097c00-e3e9-475a-ba13-62a7368d29a0)

***

## Data
The dataset has 2.4M stories. When tokenized with "bert-base-uncased", it produces ~450M tokens.
A model with ~20M parameters could benefit fully from this dataset. Bigger than this would need more than TinyStories dataset and smaller than this would not need that big of a dataset(According to Chinchilla scaling law).

***

## Dataset
https://huggingface.co/datasets/roneneldan/TinyStories

***

Paper link: https://arxiv.org/pdf/2305.07759.pdf
